{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d31394b-b3b7-4fa4-8c05-59a0493da775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\20155\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: faker in c:\\users\\20155\\anaconda3\\lib\\site-packages (36.1.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\20155\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\20155\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\20155\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\20155\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\20155\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2af4dcf2-5624-4d75-b37b-ee67d1d93cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Batch 1/21 (50000 records) added!\n",
      "✅ Batch 2/21 (50000 records) added!\n",
      "✅ Batch 3/21 (50000 records) added!\n",
      "✅ Batch 4/21 (50000 records) added!\n",
      "✅ Batch 5/21 (50000 records) added!\n",
      "✅ Batch 6/21 (50000 records) added!\n",
      "✅ Batch 7/21 (50000 records) added!\n",
      "✅ Batch 8/21 (50000 records) added!\n",
      "✅ Batch 9/21 (50000 records) added!\n",
      "✅ Batch 10/21 (50000 records) added!\n",
      "✅ Batch 11/21 (50000 records) added!\n",
      "✅ Batch 12/21 (50000 records) added!\n",
      "✅ Batch 13/21 (50000 records) added!\n",
      "✅ Batch 14/21 (50000 records) added!\n",
      "✅ Batch 15/21 (50000 records) added!\n",
      "✅ Batch 16/21 (50000 records) added!\n",
      "✅ Batch 17/21 (50000 records) added!\n",
      "✅ Batch 18/21 (50000 records) added!\n",
      "✅ Batch 19/21 (50000 records) added!\n",
      "✅ Batch 20/21 (50000 records) added!\n",
      "✅ Batch 21/21 (0 records) added!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Load existing dataset\n",
    "file_path = \"heart.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define value ranges from the dataset\n",
    "age_range = (df[\"Age\"].min(), df[\"Age\"].max())\n",
    "resting_bp_range = (df[\"RestingBP\"].min(), df[\"RestingBP\"].max())\n",
    "cholesterol_range = (df[\"Cholesterol\"].min(), df[\"Cholesterol\"].max())\n",
    "max_hr_range = (df[\"MaxHR\"].min(), df[\"MaxHR\"].max())\n",
    "oldpeak_range = (df[\"Oldpeak\"].min(), df[\"Oldpeak\"].max())\n",
    "\n",
    "# Get unique categorical values\n",
    "sex_values = df[\"Sex\"].unique().tolist()\n",
    "chest_pain_values = df[\"ChestPainType\"].unique().tolist()\n",
    "resting_ecg_values = df[\"RestingECG\"].unique().tolist()\n",
    "exercise_angina_values = df[\"ExerciseAngina\"].unique().tolist()\n",
    "st_slope_values = df[\"ST_Slope\"].unique().tolist()\n",
    "\n",
    "fake = Faker()\n",
    "\n",
    "# Define total new records and batch size\n",
    "num_new_samples = 1000000  # Total records to generate\n",
    "batch_size = 50000  # Records per batch\n",
    "num_batches = num_new_samples // batch_size  # Number of full batches\n",
    "\n",
    "# Connect to the database\n",
    "conn = sqlite3.connect(\"database.sqlite\")\n",
    "\n",
    "for batch in range(num_batches + 1):  \n",
    "    batch_size_current = batch_size if batch < num_batches else num_new_samples % batch_size  # Handle last batch size\n",
    "\n",
    "    # Generate synthetic data batch\n",
    "    synthetic_data = {\n",
    "        \"Age\": [random.randint(*age_range) for _ in range(batch_size_current)],\n",
    "        \"Sex\": [random.choice(sex_values) for _ in range(batch_size_current)],\n",
    "        \"ChestPainType\": [random.choice(chest_pain_values) for _ in range(batch_size_current)],\n",
    "        \"RestingBP\": [random.randint(*resting_bp_range) for _ in range(batch_size_current)],\n",
    "        \"Cholesterol\": [random.randint(*cholesterol_range) for _ in range(batch_size_current)],\n",
    "        \"FastingBS\": [random.choice([0, 1]) for _ in range(batch_size_current)],\n",
    "        \"RestingECG\": [random.choice(resting_ecg_values) for _ in range(batch_size_current)],\n",
    "        \"MaxHR\": [random.randint(*max_hr_range) for _ in range(batch_size_current)],\n",
    "        \"ExerciseAngina\": [random.choice(exercise_angina_values) for _ in range(batch_size_current)],\n",
    "        \"Oldpeak\": [round(random.uniform(*oldpeak_range), 1) for _ in range(batch_size_current)],\n",
    "        \"ST_Slope\": [random.choice(st_slope_values) for _ in range(batch_size_current)],\n",
    "        \"HeartDisease\": [random.choice([0, 1]) for _ in range(batch_size_current)]\n",
    "    }\n",
    "\n",
    "    df_synthetic = pd.DataFrame(synthetic_data)\n",
    "\n",
    "    # Append batch to the database\n",
    "    df_synthetic.to_sql(\"heart_data\", conn, if_exists=\"append\", index=False)\n",
    "\n",
    "    print(f\"✅ Batch {batch+1}/{num_batches+1} ({batch_size_current} records) added!\")\n",
    "\n",
    "conn.close()  # Close the database connection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1042d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data deleted successfully!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"database.sqlite\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute(\"DELETE FROM heart_data\")  \n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(\"✅ Data deleted successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54048e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ source.sqlite created with both tables!\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "conn_source = sqlite3.connect(\"source.sqlite\")\n",
    "\n",
    "\n",
    "df_original = pd.read_csv(\"heart.csv\")\n",
    "df_original.to_sql(\"original_data\", conn_source, if_exists=\"replace\", index=False)\n",
    "\n",
    "\n",
    "conn_old = sqlite3.connect(\"database.sqlite\")\n",
    "df_synthetic = pd.read_sql(\"SELECT * FROM heart_data\", conn_old)\n",
    "conn_old.close()\n",
    "\n",
    "df_synthetic.to_sql(\"synthetic_data\", conn_source, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn_source.close()\n",
    "\n",
    "print(\"✅ source.sqlite created with both tables!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a50ca222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect(\"raw.sqlite\")\n",
    "df_synthetic.head(0).to_sql(\"raw_synthetic\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "\n",
    "print(\"done\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e715c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Transferred 50000 records (Last ID: 100000)\n",
      " Transferred 50000 records (Last ID: 150000)\n",
      " Transferred 50000 records (Last ID: 200000)\n",
      " Transferred 50000 records (Last ID: 250000)\n",
      " Transferred 50000 records (Last ID: 300000)\n",
      " Transferred 50000 records (Last ID: 350000)\n",
      " Transferred 50000 records (Last ID: 400000)\n",
      " Transferred 50000 records (Last ID: 450000)\n",
      " Transferred 50000 records (Last ID: 500000)\n",
      " Transferred 50000 records (Last ID: 550000)\n",
      " Transferred 50000 records (Last ID: 600000)\n",
      " Transferred 50000 records (Last ID: 650000)\n",
      " Transferred 50000 records (Last ID: 700000)\n",
      " Transferred 50000 records (Last ID: 750000)\n",
      " Transferred 50000 records (Last ID: 800000)\n",
      " Transferred 50000 records (Last ID: 850000)\n",
      " Transferred 50000 records (Last ID: 900000)\n",
      " Transferred 50000 records (Last ID: 950000)\n",
      " Transferred 50000 records (Last ID: 1000000)\n",
      "✅ No more records to transfer. Process completed.\n",
      " Batch processing complete! All synthetic data moved to raw.sqlite\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "batch_size = 50000  \n",
    "\n",
    "conn_source = sqlite3.connect(\"source.sqlite\")\n",
    "conn_raw = sqlite3.connect(\"raw.sqlite\")\n",
    "\n",
    "\n",
    "cursor_source = conn_source.cursor()\n",
    "cursor_source.execute(\"PRAGMA table_info(synthetic_data)\")\n",
    "columns = [row[1] for row in cursor_source.fetchall()]\n",
    "columns_str = \", \".join(columns)\n",
    "\n",
    "conn_raw.execute(f\"CREATE TABLE IF NOT EXISTS raw_synthetic ({columns_str})\")\n",
    "\n",
    "\n",
    "if \"id\" in columns:\n",
    "    primary_key_column = \"id\"\n",
    "else:\n",
    "    primary_key_column = \"rowid\"\n",
    "\n",
    "cursor_raw = conn_raw.cursor()\n",
    "cursor_raw.execute(f\"SELECT MAX({primary_key_column}) FROM raw_synthetic\")\n",
    "last_transferred_id = cursor_raw.fetchone()[0] or 0\n",
    "\n",
    "while True:\n",
    "    query = f\"\"\"\n",
    "    SELECT {primary_key_column}, * FROM synthetic_data \n",
    "    WHERE {primary_key_column} > {last_transferred_id} \n",
    "    ORDER BY {primary_key_column} LIMIT {batch_size}\n",
    "    \"\"\"\n",
    "    df_batch = pd.read_sql(query, conn_source)\n",
    "\n",
    "    if df_batch.empty:\n",
    "        print(\"✅ No more records to transfer. Process completed.\")\n",
    "        break  \n",
    "\n",
    "    df_batch.to_sql(\"raw_synthetic\", conn_raw, if_exists=\"append\", index=False)\n",
    "\n",
    "    \n",
    "    cursor_raw.execute(f\"\"\"\n",
    "    SELECT COUNT(*) FROM raw_synthetic \n",
    "    WHERE {primary_key_column} BETWEEN {df_batch[primary_key_column].min()} \n",
    "    AND {df_batch[primary_key_column].max()}\n",
    "    \"\"\")\n",
    "    transferred_count = cursor_raw.fetchone()[0]\n",
    "\n",
    "    if transferred_count < len(df_batch):\n",
    "        print(f\"⚠️ Error: Some records were lost. Retrying batch (Last ID: {last_transferred_id})\")\n",
    "        continue  \n",
    "    \n",
    "    last_transferred_id = df_batch[primary_key_column].max()\n",
    "    print(f\" Transferred {len(df_batch)} records (Last ID: {last_transferred_id})\")\n",
    "\n",
    "conn_source.close()\n",
    "conn_raw.close()\n",
    "\n",
    "print(\" Batch processing complete! All synthetic data moved to raw.sqlite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e61189",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
